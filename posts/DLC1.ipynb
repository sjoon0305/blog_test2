{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"딥러닝 Course1 by Andrew ng\"\n",
    "author: \"유성준\"\n",
    "date: \"01/04/2024\"\n",
    "categories: [Machine Learning, Deep Learnig, Python] \n",
    "---\n",
    "![](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FOwS8B%2Fbtq0OjM6g8l%2FqslZkb00YMJV9nUik6t1G1%2Fimg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C1W1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- X(입력)와 Y(출력)를 연결지어주는 함수를 찾는 과정\n",
    "- 데이터가 많으면 많을수록 성능이 좋은 함수를 찾을 수 있음\n",
    "- 해당 뉴런에 관련 없는 입력값이라도 입력해야 함\n",
    "- **그 입력의 관계 여부, 가중치는 학습하면서 조절됨**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NN 종류\n",
    "- NN: 데이터베이스화된 데이터에 적합\n",
    "- CNN: 이미지에 적함\n",
    "- RNN: 오디오, 텍스트에 적합\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data 종류\n",
    "- 정형 데이터\n",
    "    - 데이터베이스로 표현 가능\n",
    "    - 정보의 특성 확정\n",
    "- 비정형 데이터\n",
    "    - 오디오, 텍스트, 이미지\n",
    "    - 특징값을 추출하기 어려움\n",
    "    - 딥러닝 기술 발전으로 판별 가능"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 최근에 들어 딥러닝이 발전한 계기\n",
    "- 디지털 정보량의 증가, **컴퓨터 성능 향상**, 알고리즘 혁신\n",
    "- Sigmoid --> ReLU로 activation function을 바꾸어 학습 속도 향상"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C1W2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 신경망 학습방법\n",
    "- 정방향 전파\n",
    "- 역방향 전파"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary Classification(이진 분류)\n",
    "- 1 or 0로 분류하는 것\n",
    "- 연체를 했다 / 연체를 하지 않았다.\n",
    "- 로지스틱 회귀(Logistic regression) 알고리즘 사용\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 표기\n",
    "- $m$: 학습을 위한 데이터 세트 수\n",
    "- ${(x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), ... (x^{(m)}, y^{(m)}), }$\n",
    "\n",
    "\n",
    "- $n$: 입력 데이터 하나의 원소 개수\n",
    "- $x$: 입력 데이터 하나\n",
    "- $X$: 입력 데이터\n",
    "- $X = \\begin{bmatrix}\n",
    "  x^{(1)}_1 & x^{(2)}_1 & \\cdots & x^{(m)}_1 \\\\\n",
    "  \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "  x^{(1)}_n & x^{(2)}_n & \\cdots & x^{(m)}_n\n",
    "\\end{bmatrix}$\n",
    "- X.shape = (n, m)\n",
    "\n",
    "\n",
    "- $y$: 출력 데이터 하나\n",
    "- $Y$: 출력 데이터\n",
    "- $Y = [y^{(1)}, y^{(2)}, ... y^{(n)}]$\n",
    "- Y.shape = (1, m)\n",
    "\n",
    "\n",
    "- $\\hat{y}$: 예측값\n",
    "- 0과 1 사이의 확률값으로 나타남\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression(로지스틱 회귀)\n",
    "\n",
    "![](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTwW1UGCC_OVB6IxMKkgUt2yftdHy4I2FqiMQ&usqp=CAU)\n",
    "\n",
    "- 입력 특성(x)에 대한 실제값(y)을 가지고 예측값($\\hat{y}$)을 구하고 그 예측값과 실제값의 오차가 최소가 되도록 하는 파라미터($W$, $b$)를 구해야 함\n",
    "\n",
    "- $W$: $x$와 크기가 같은 $n$차원의 벡터\n",
    "- $b$: 상수\n",
    "- 예측값은 아래와 같이 구함\n",
    "\n",
    "- $\\hat{y} = \\sigma(W^TX+b)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid function\n",
    "\n",
    "\n",
    "- $\\sigma(z) = {1 \\over 1 + e^{-z}}$\n",
    "    - $z$가 클 수록 1로 수렴\n",
    "    - $z$가 작을 수록 0으로 수렴\n",
    "- 위 식에서 $\\sigma$가 하는 역할은 예측값이 0에서 1사이가 되도록 만드는 역할\n",
    "\n",
    "- Sigmoid 함수 그래프"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2Fcb3bEH%2Fbtq0UdlTEZz%2FfDeJKAzPlYkSs011P0fPt0%2Fimg.png)\n",
    "\n",
    "- $\\hat{y}$은 항상 0에서 1사이의 값을 가진다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function(손실 함수)\n",
    "- 한 세트에 대한 예측값($\\hat{y}$)과 실제값($y$)의 오차를 구하는 함수\n",
    "\n",
    "- $L(y, \\hat{y}) = -(y\\log{\\hat{y}}+(1-y)\\log(1-\\hat{y}))$\n",
    "- 실제값($y$)이 0이냐 1이냐에 따라서 오차를 구하는 식이 달라진다.\n",
    "- $y=0$일 때: $L(y, \\hat{y}) = -y\\log{(1-\\hat{y})}$\n",
    "- $y=1$일 때: $L(y, \\hat{y}) = -y\\log{\\hat{y}}$\n",
    "- 그래프로 표현\n",
    "- ![](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FbsVKEL%2Fbtq0XQcqf19%2FdLQ59SVLirQbpARynqCPC0%2Fimg.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost Function(비용 함수)\n",
    "- 모든 입력세트에 대한 오차를 구하는 함수\n",
    "\n",
    "- $J(W, b) = -{1 \\over m}\\sum_{i=1}^m(y^{(i)}\\log{\\hat{y}^{(i)}}+(1-y^{(i)})\\log(1-\\hat{y}^{(i)}))$\n",
    "\n",
    "- 로지스틱 회귀 모델을 학습한다는 것은 비용 함수 $J$를 최소로 만드는 $W$와 $b$를 찾는 것을 의미"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 경사 하강법\n",
    "- 비용함수의 값을 최소화하는 $w$와 $b$를 찾는데 사용할 수 있는 방법이다. 이때 비용함수는 볼록한(convex) 형태여야 한다. 만약 비용함수의 형태가 볼록하지 않다면 지역 최솟값을 여러 개 가지게 되어 진짜 최솟값을 찾기 어려워진다.\n",
    "- 비용함수의 최솟값을 찾기 위한 시작점은 임의로 정하여도 상관없다. 경사 하강법을 사용하면 어디에서 시작하든 최솟값이 있는 곳으로 향하게 된다. 가파른 방향으로 한 스텝씩 업데이트하며 최솟값을 찾아간다.\n",
    "- $w:=w-\\alpha{\\partial{J(w,b)}\\over\\partial{d}}$\n",
    "- $b:=b-\\alpha{\\partial{J(w,b)}\\over\\partial{d}}$\n",
    "\n",
    "![](https://builtin.com/sites/www.builtin.com/files/styles/ckeditor_optimize/public/inline-images/national/gradient-descent-convex-function.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 로지스틱 회귀의 경사 하강법에서 for 문이 알고리즘을 비효율적으로 만듬 -> 벡터화를 통해 명시적 for 문을 제거"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 벡터화"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 벡터화를 사용하여 동시에 분산 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250003.57277653238\n",
      "Vectorized version: 0.49757957458496094ms\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "\n",
    "a = np.random.rand(1000000)\n",
    "b = np.random.rand(1000000)\n",
    "\n",
    "tic = time.time()\n",
    "c = np.dot(a, b)\n",
    "toc = time.time()\n",
    "\n",
    "print(c)\n",
    "print(\"Vectorized version: \" + str(1000*(toc-tic)) + \"ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- for 문을 사용해서 순차적으로 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250003.5727765315\n",
      "for loop: 353.87134552001953ms\n"
     ]
    }
   ],
   "source": [
    "c = 0\n",
    "tic = time.time()\n",
    "for i in range(1000000):\n",
    "    c += a[i]*b[i]\n",
    "toc = time.time()\n",
    "\n",
    "print(c)\n",
    "print(\"for loop: \" + str(1000*(toc-tic)) + \"ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 즉 벡터화를 사용한 코드의 시간이 훨씬 빠르다(0.49ms<<353.87ms) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 파이썬 브로드캐스팅\n",
    "- 각 식자재 100g당 영양소가 가지는 칼로리 표"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|            | Apples | Beef  | Eggs | Potatoes |\n",
    "|------------|--------|-------|------|----------|\n",
    "| Carb       | 56.0   | 0.0   | 4.4  | 68.0     |\n",
    "| Protein    | 1.2    | 104.0 | 52.0 | 8.0      |\n",
    "| Fat        | 1.8    | 135.0 | 99.0 | 0.9      |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 브로딩캐스팅을 이용해 식자재 총 칼로리 중 각 영양소가 차지하는 비율을 구하는 예제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[94.91525424  0.          2.83140283 88.42652796]\n",
      " [ 2.03389831 43.51464435 33.46203346 10.40312094]\n",
      " [ 3.05084746 56.48535565 63.70656371  1.17035111]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "A = np.array([[56, 0, 4.4, 68],\n",
    "[1.2, 104, 52, 8],\n",
    "[1.8, 135, 99, 0.9]])\n",
    "\n",
    "cal = np.sum(A, axis=0)\n",
    "percentage = 100*A/cal.reshape(1, 4)\n",
    "print(percentage)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
